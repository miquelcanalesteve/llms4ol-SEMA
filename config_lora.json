{
    "Llama": {
        "full": [
            "self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "self_attn.o_proj",
            "mlp.gate_proj", "mlp.up_proj", "mlp.down_proj"
        ],
        "minimal": [
            "self_attn.q_proj", "self_attn.v_proj"
        ]
    }
}
